# -*- coding: utf-8 -*-
"""Skinterest1A.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QuP4CYtRjhig5BB_G5wON4TS4deT_KjO

# Milestone 1
"""

'''
from google.colab import drive
drive.mount('/content/drive')
'''

# import pandas as pd

# base_path = "/content/drive/Shared drives/BTT:Skinterest Team 1A/Data"

# reviews_1 = pd.read_csv(f"{base_path}/reviews_0-250_masked.csv")
# reviews_2 = pd.read_csv(f"{base_path}/reviews_250-500_masked.csv")
# reviews_3 = pd.read_csv(f"{base_path}/reviews_500-750_masked.csv")
# reviews_4 = pd.read_csv(f"{base_path}/reviews_750-1250_masked.csv")
# reviews_5 = pd.read_csv(f"{base_path}/reviews_1250-end_masked.csv")

# reviews_1.head()
# reviews_2.head()
# reviews_3.head()
# reviews_4.head()
# reviews_5.head()

"""### Task 1: Text Cleaning"""

import pandas as pd

reviews_1 = pd.read_csv("/content/reviews_0-250_masked.csv")
reviews_2 = pd.read_csv("/content/reviews_250-500_masked.csv")
reviews_3 = pd.read_csv("/content/reviews_500-750_masked.csv")
reviews_4 = pd.read_csv("/content/reviews_750-1250_masked.csv")
reviews_5 = pd.read_csv("/content/reviews_1250-end_masked.csv")

reviews_1.head()
reviews_2.head()
reviews_3.head()
reviews_4.head()
reviews_5.head()

import re
import nltk

def clean_text(text):
    text = str(text).lower()                              # lowercase
    text = re.sub(r"<.*?>", "", text)                     # remove HTML
    text = re.sub(r"[^a-zA-Z0-9\s.,!?]", "", text)        # Remove special characters (keep letters, numbers, spaces, and basic punctuation if needed)
    text = text.encode("ascii", "ignore").decode()        # Remove non-ASCII characters
    text = re.sub(r"\s+", " ", text).strip()              # Normalize spaces
    return text



reviews_1["review_title"] = reviews_1["review_title"].apply(clean_text)
reviews_2["review_title"] = reviews_2["review_title"].apply(clean_text)
reviews_3["review_title"] = reviews_3["review_title"].apply(clean_text)
reviews_4["review_title"] = reviews_4["review_title"].apply(clean_text)
reviews_5["review_title"] = reviews_5["review_title"].apply(clean_text)

'''
from google.colab import drive
drive.mount('/content/drive')
'''

reviews_1.head()

import re

def verify_cleaning(df, column):
    """
    Verify if any HTML tags, non-ASCII chars, or disallowed special characters remain
    in a given DataFrame column. Prints summary and examples.
    """
    html_pattern = re.compile(r"<.*?>")
    non_ascii_pattern = re.compile(r"[^\x00-\x7F]")   # anything outside ASCII
    special_pattern = re.compile(r"[^a-zA-Z0-9\s.,!?]")

    # Check each type of violation
    html_violations = df[column].astype(str).str.contains(html_pattern).sum()
    non_ascii_violations = df[column].astype(str).str.contains(non_ascii_pattern).sum()
    special_violations = df[column].astype(str).str.contains(special_pattern).sum()

    print(f"\nResults for column '{column}':")
    print(f"  HTML tags left: {html_violations}")
    print(f"  Non-ASCII chars left: {non_ascii_violations}")
    print(f"  Disallowed special chars left: {special_violations}")

    # Optionally show examples if violations exist
    if html_violations > 0:
        print("  Sample HTML violation:", df[df[column].str.contains(html_pattern, na=False)][column].head(3).tolist())
    if non_ascii_violations > 0:
        print("  Sample non-ASCII violation:", df[df[column].str.contains(non_ascii_pattern, na=False)][column].head(3).tolist())
    if special_violations > 0:
        print("  Sample special char violation:", df[df[column].str.contains(special_pattern, na=False)][column].head(3).tolist())

for i, df in enumerate([reviews_1, reviews_2, reviews_3, reviews_4, reviews_5], start=1):
    verify_cleaning(df, "review_title")

"""### Task 2: Review Validation"""

import glob

# Load and combine all review CSVs
files = sorted(glob.glob("/content/reviews_*_masked.csv"))
files = glob.glob("/content/reviews_*_masked.csv")
print("Files found:", files)

# df has all review CSVs
df = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)

before = len(df)

# Remove duplicates
df = df.drop_duplicates()

after = len(df)

print(f"Rows before removing duplicates: {before}")
print(f"Rows after removing duplicates:  {after}")
print(f"Duplicates removed: {before - after}")

# Filter out reviews with less than 10 words

# Create a new column counting words in each review
df["word_count"] = (
    df["review_text"]
    .fillna("")           # Treat NaN as empty string
    .astype(str)          # Ensure it's a string
    .str.split()          # Split by whitespace
    .apply(len)           # Count words
)


# Get number of rows before filtering
before = df.shape[0]

# Keep only reviews with 10 or more words
df = df[df["word_count"] >= 10].copy()

# Rows after filtering
after = df.shape[0]

print(f"Rows before filtering: {before}")
print(f"Rows after filtering: {after}")
print(f"Short reviews removed: {before - after}")

# Delete word_count column after filtering
df = df.drop(columns=["word_count"])

# Filter out potential spam content

# Build a simple regex: links, emails, promo phrases, "click here/buy now"
simple_spam_re = r"(http[s]?://|www\.|@\w+|\b(use code|promo code|discount|coupon|click here|buy now|free shipping)\b)"

# Flag rows whose text matches any of the above
is_spam = df["review_text"].astype(str).str.contains(simple_spam_re, case=False, na=False)

# Remove them
before = len(df)
df = df[~is_spam].copy()
after = len(df)

print(f"Removed {before - after} potential spam reviews (kept {after}).")



"""### Task 3: Missing Data Handling

"""

print(df.isnull().sum())

# 1. Drop redundant index columns
df = df.drop(columns=["Unnamed: 0.1", "Unnamed: 0"])

# 2. Fill review_title with placeholder
missing_titles = df["review_title"].isna().sum()
df["review_title"] = df["review_title"].fillna("untitled")
print(f"Filled {missing_titles} missing review titles with 'untitled'.")

# 3. Fill demographic attributes with 'Unknown'
for col in ["skin_tone", "eye_color", "skin_type", "hair_color"]:
    missing_vals = df[col].isna().sum()
    df[col] = df[col].fillna("Unknown")
    print(f"Filled {missing_vals} missing values in '{col}' with 'Unknown'.")

# 4. Leave 'is_recommended' and 'helpfulness' as NaN (document decision)
print(f"'is_recommended' missing: {df['is_recommended'].isna().sum()}")
print(f"'helpfulness' missing: {df['helpfulness'].isna().sum()}")

# 5. Verify remaining missingness
print("\nRemaining missing values after cleaning:")
print(df.isnull().sum())

"""### Task 4: Text Standardization"""

#Clean_text function has already converted to lowercase and normalized whitespace

#Contraction dictionary from
#https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python
#apostrophes removed so it works on cleaned text

contractions = {
    "aint": "am not",
    "arent": "are not",
    "cant": "cannot",
    "couldnt": "could not",
    "didnt": "did not",
    "doesnt": "does not",
    "dont": "do not",
    "hadnt": "had not",
    "hasnt": "has not",
    "havent": "have not",
    "hed": "he would",
    "hell": "he will",
    "hes": "he is",
    "id": "i would",
    "ill": "i will",
    "im": "i am",
    "ive": "i have",
    "isnt": "is not",
    "itd": "it would",
    "itll": "it will",
    "its": "it is",
    "lets": "let us",
    "shes": "she is",
    "shouldnt": "should not",
    "thats": "that is",
    "theyre": "they are",
    "theyve": "they have",
    "wasnt": "was not",
    "were": "we are",
    "weve": "we have",
    "werent": "were not",
    "whats": "what is",
    "wheres": "where is",
    "whos": "who is",
    "wont": "will not",
    "wouldnt": "would not",
    "yall": "you all",
    "youre": "you are",
    "youve": "you have"
}


#function to expand contractions
def expand_contractions(text):
    if not isinstance(text, str):
        return text
    words = text.split()
    expanded_words = [contractions.get(word, word) for word in words]
    return " ".join(expanded_words)

#loop to replace contractions with expansions
for df in [reviews_1, reviews_2, reviews_3, reviews_4, reviews_5]:
  for col in ["review_title", "review_text"]:
    df[col] = df[col].apply(clean_text)
    df[col] = df[col].apply(expand_contractions)

#check datasets for expanded contractions



print((reviews_1[["review_title", "review_text"]].head(10)))
print((reviews_2[["review_title", "review_text"]].head(10)))
print((reviews_3[["review_title", "review_text"]].head(10)))
print((reviews_4[["review_title", "review_text"]].head(10)))
print((reviews_5[["review_title", "review_text"]].head(10)))

"""### Task 5: Feature Engineering"""

import pandas as pd
import numpy as np
from textblob import TextBlob

# review length features
df["review_length_words"] = df["review_text"].fillna("").astype(str).str.split().apply(len)
df["review_length_chars"] = df["review_text"].fillna("").astype(str).str.len()
df["avg_word_length"] = np.where(
    df["review_length_words"] > 0,
    (df["review_text"].fillna("").astype(str).str.replace(r"\s+", "", regex=True).str.len()) / df["review_length_words"],
    0.0
)

# sentiment score using TextBlob
def get_sentiment(text):
    if not isinstance(text, str) or not text.strip():
        return 0.0
    return TextBlob(text).sentiment.polarity  # returns value in [-1, 1]

df["sentiment_score"] = df["review_text"].fillna("").astype(str).apply(get_sentiment)

# map sentiment score to 1–5 scale and compute discrepancy
df["sentiment_rating"] = 2.0 * df["sentiment_score"] + 3.0
df["rating_discrepancy"] = df["sentiment_rating"] - df["rating"].astype(float)

# temporal features
df["submission_time"] = pd.to_datetime(df["submission_time"], errors="coerce", utc=True)
df["rev_year"] = df["submission_time"].dt.year
df["rev_quarter"] = df["submission_time"].dt.quarter
df["rev_month"] = df["submission_time"].dt.month
df["rev_week"] = df["submission_time"].dt.isocalendar().week.astype("Int64")
df["rev_day"] = df["submission_time"].dt.day
df["rev_hour"] = df["submission_time"].dt.hour
df["rev_dayofweek"] = df["submission_time"].dt.dayofweek
df["is_weekend"] = df["rev_dayofweek"].isin([5, 6]).astype(int)

if "product_id" in df.columns:
    first_ts_per_product = df.groupby("product_id")["submission_time"].transform("min")
    df["days_since_first_review_product"] = (df["submission_time"] - first_ts_per_product).dt.total_seconds() / 86400.0
else:
    df["days_since_first_review_product"] = np.nan

global_first = df["submission_time"].min()
df["days_since_global_first"] = (df["submission_time"] - global_first).dt.total_seconds() / 86400.0

print(df.head())

"""### Task 6: Sentiment Analysis"""

# Task 6: Sentiment Analysis → Emotional tone


def label_sentiment(score, pos_threshold=0.1, neg_threshold=-0.1):
    if score >= pos_threshold:
        return "Positive"
    elif score <= neg_threshold:
        return "Negative"
    else:
        return "Neutral"

#added sentiment score to each review
df["sentiment_label"] = df["sentiment_score"].apply(label_sentiment)

print("Sentiment distribution:")
print(df["sentiment_label"].value_counts())

print(df[["review_text", "sentiment_score", "sentiment_label"]].head(10))

"""### Task 7: Sentiment Analysis"""

import pandas as pd
import numpy as np

# summarize sentiment by group (brand or product)
def sentiment_summary(df, key, min_reviews=20):
    x = df.copy()

    # create label if not already there
    if "sentiment_label" not in x.columns:
        def label_sentiment(s, pos=0.1, neg=-0.1):
            if pd.isna(s): return "Neutral"
            if s >= pos: return "Positive"
            if s <= neg: return "Negative"
            return "Neutral"
        x["sentiment_label"] = x["sentiment_score"].apply(label_sentiment)

    # group + stats
    agg = (
        x.groupby(key, dropna=False)
         .agg(
             n_reviews=("sentiment_score", "size"),
             mean_score=("sentiment_score", "mean"),
             pos_rate=("sentiment_label", lambda s: np.mean(s=="Positive")),
             neg_rate=("sentiment_label", lambda s: np.mean(s=="Negative")),
             neu_rate=("sentiment_label", lambda s: np.mean(s=="Neutral")),
         )
         .reset_index()
    )

    # filter by review count
    agg = agg[agg["n_reviews"] >= min_reviews].sort_values("mean_score", ascending=False)
    return agg

results = {}

# by brand
if "brand" in df.columns:
    brand_stats = sentiment_summary(df, "brand", min_reviews=20)
    results["brand_stats"] = brand_stats
    print("Top 10 brands by sentiment:")
    print(brand_stats.head(10)[["brand","n_reviews","mean_score","pos_rate","neg_rate"]])
    print("\nBottom 10 brands:")
    print(brand_stats.tail(10)[["brand","n_reviews","mean_score","pos_rate","neg_rate"]])

# by product
if "product_name" in df.columns:
    key = "product_name"
elif "product_id" in df.columns:
    key = "product_id"
else:
    key = None


if key:
    product_stats = sentiment_summary(df, key, min_reviews=20)
    results["product_stats"] = product_stats
    print(f"\nTop 10 products ({key}):")
    print(product_stats.head(10)[[key,"n_reviews","mean_score","pos_rate","neg_rate"]])
    print(f"\nBottom 10 products ({key}):")
    print(product_stats.tail(10)[[key,"n_reviews","mean_score","pos_rate","neg_rate"]])

import matplotlib.pyplot as plt

# Top 10 products barplot
top10 = product_stats.head(10)
plt.barh(top10["product_name"], top10["mean_score"], color="green")
plt.xlabel("Mean Sentiment Score")
plt.title("Top 10 Products by Sentiment")
plt.gca().invert_yaxis()
plt.show()

# Bottom 10 products barplot
bottom10 = product_stats.tail(10)
plt.barh(bottom10["product_name"], bottom10["mean_score"], color="red")
plt.xlabel("Mean Sentiment Score")
plt.title("Bottom 10 Products by Sentiment")
plt.gca().invert_yaxis()
plt.show()

"""# Milestone 2

### Task 1: Build Customer-Level Dataset

> Add blockquote
"""

import pandas as pd
import numpy as np

# drop old index columns from the CSVs
df = df.loc[:, ~df.columns.str.contains("^Unnamed")]

# Merge with product info
try:
    product_info = pd.read_csv("product_info.csv")
except:
    product_info = pd.read_csv("product_info_skincare.csv")

df = df.merge(product_info, on="product_id", how="left")

# Combine duplicate columns (from merge)
def coalesce(two_cols, new_name):
    a, b = two_cols
    if a in df.columns and b in df.columns:
        df[new_name] = df[a].where(df[a].notna(), df[b])
    elif a in df.columns:
        df[new_name] = df[a]
    elif b in df.columns:
        df[new_name] = df[b]

coalesce(("rating_x","rating_y"), "rating")
coalesce(("brand_name_x","brand_name_y"), "brand_name")
coalesce(("product_name_x","product_name_y"), "product_name")
coalesce(("price_usd_x","price_usd_y"), "price_usd")

# drop the suffixed columns now
df = df.drop(columns=[c for c in df.columns if c.endswith(("_x","_y"))], errors="ignore")

# Ensure essential types
df["submission_time"] = pd.to_datetime(df["submission_time"], errors="coerce", utc=True)
df["product_id"] = df["product_id"].astype(str)
df["rating"] = pd.to_numeric(df["rating"], errors="coerce")

# treat each row as a customer record
customer_df = df.copy()

# set avg_rating from rating (since 1 review per user)
customer_df["avg_rating"] = customer_df["rating"]

# recency feature
latest = customer_df["submission_time"].max()
customer_df["recency_days"] = (latest - customer_df["submission_time"]).dt.total_seconds() / 86400.0

print("Columns after cleaning:\n", customer_df.columns.tolist())
print("Final shape:", customer_df.shape)
customer_df.head()

"""### Task 2: Customer segmentation (Clustering)"""

'''

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.metrics import silhouette_score, davies_bouldin_score
import matplotlib.pyplot as plt

# Select numeric features for clustering
num_cols = [
    "avg_rating", "recency_days", "helpfulness",
    "total_feedback_count", "total_pos_feedback_count",
    "total_neg_feedback_count", "price_usd"
]

features = [c for c in num_cols if c in customer_df.columns]
X = customer_df[features].dropna()

print(f"Using {len(features)} numeric features: {features}")
print("Shape before scaling:", X.shape)

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Run K-means for k = 3–15 and track metrics
silhouette_scores = []
db_scores = []

for k in range(3, 16):
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X_scaled)

    sil = silhouette_score(X_scaled, labels)
    db = davies_bouldin_score(X_scaled, labels)

    silhouette_scores.append(sil)
    db_scores.append(db)

    print(f"k={k}: Silhouette={sil:.3f}, Davies–Bouldin={db:.3f}")

# Pick best k (highest silhouette)
best_k = range(3, 16)[np.argmax(silhouette_scores)]
print(f"\nBest K (by silhouette): {best_k}")

''' #just testing something below with your code
'''

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.metrics import silhouette_score, davies_bouldin_score
import matplotlib.pyplot as plt


# 1. Select numeric features for clustering

num_cols = [
    "avg_rating", "recency_days", "helpfulness",
    "total_feedback_count", "total_pos_feedback_count",
    "total_neg_feedback_count", "price_usd"
]

features = [c for c in num_cols if c in customer_df.columns]
X = customer_df[features].dropna()

print(f"Using {len(features)} numeric features: {features}")
print("Shape before scaling:", X.shape)

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)


# 2. Run K-means for k = 3–15 and track metrics

silhouette_scores = []
db_scores = []
k_values = range(3, 16)

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X_scaled)

    sil = silhouette_score(X_scaled, labels)
    db = davies_bouldin_score(X_scaled, labels)

    silhouette_scores.append(sil)
    db_scores.append(db)

    print(f"k={k:2d} | Silhouette={sil:.3f} | Davies–Bouldin={db:.3f}")


# 3. Pick best k (using combined metric)

sil_norm = (silhouette_scores - np.min(silhouette_scores)) / np.ptp(silhouette_scores)
db_norm = (db_scores - np.min(db_scores)) / np.ptp(db_scores)
combined_score = sil_norm - db_norm
best_k = k_values[np.argmax(combined_score)]

print(f"\nBest K (combined silhouette + Davies–Bouldin): {best_k}")

# 4. Visualize metrics

plt.figure(figsize=(10, 5))
plt.plot(k_values, silhouette_scores, marker='o', label='Silhouette (↑ better)')
plt.plot(k_values, db_scores, marker='o', label='Davies–Bouldin (↓ better)')
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Score")
plt.title("K-Means Clustering Metrics")
plt.legend()
plt.grid(True, linestyle='--', alpha=0.6)
plt.show()


# 5. Run final K-Means with chosen k and assign clusters

kmeans_final = KMeans(n_clusters=best_k, random_state=42, n_init=10)
customer_df["cluster_id"] = kmeans_final.fit_predict(X_scaled)

print("\nCluster distribution (K-Means):")
print(customer_df["cluster_id"].value_counts())


# 6. Try Hierarchical (Agglomerative) Clustering with Ward linkage

agg = AgglomerativeClustering(n_clusters=best_k, linkage='ward')
agg_labels = agg.fit_predict(X_scaled)

sil_agg = silhouette_score(X_scaled, agg_labels)
db_agg = davies_bouldin_score(X_scaled, agg_labels)

print(f"\nAgglomerative Clustering (Ward linkage):")
print(f"Silhouette={sil_agg:.3f}, Davies–Bouldin={db_agg:.3f}")

# Assign hierarchical cluster IDs too
customer_df["ward_cluster_id"] = agg_labels

# 7. Summary

print("\n Clustering complete!")
print(f"- Optimal K (K-Means): {best_k}")
print("- Final cluster IDs stored in `cluster_id` (K-Means) and `ward_cluster_id` (Hierarchical).")
''' #code works but is slow
'''
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.metrics import silhouette_score, davies_bouldin_score
import matplotlib.pyplot as plt

# --- 1. Select numeric features ---
num_cols = [
    "avg_rating", "recency_days", "helpfulness",
    "total_feedback_count", "total_pos_feedback_count",
    "total_neg_feedback_count", "price_usd"
]

features = [c for c in num_cols if c in customer_df.columns]
X = customer_df[features].dropna()

# ↓↓↓ TEST MODE: Use only a subset of data ↓↓↓
X = X.sample(n=min(5000, len(X)), random_state=42)  # <-- use only 5k rows
print(f"Using {len(features)} numeric features: {features}")
print("Sampled shape before scaling:", X.shape)

# --- 2. Scale ---
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# --- 3. Run KMeans for a small range ---
silhouette_scores, db_scores = [], []
k_values = range(3, 7)  # quick test: 3–6 clusters

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=5)
    labels = kmeans.fit_predict(X_scaled)
    sil = silhouette_score(X_scaled, labels)
    db = davies_bouldin_score(X_scaled, labels)
    silhouette_scores.append(sil)
    db_scores.append(db)
    print(f"k={k:2d} | Silhouette={sil:.3f} | Davies–Bouldin={db:.3f}")

# --- 4. Choose best K ---
sil_norm = (silhouette_scores - np.min(silhouette_scores)) / np.ptp(silhouette_scores)
db_norm = (db_scores - np.min(db_scores)) / np.ptp(db_scores)
combined_score = sil_norm - db_norm
best_k = k_values[np.argmax(combined_score)]

print(f"\nBest K (combined silhouette + Davies–Bouldin): {best_k}")

# --- 5. Optional Plot ---
plt.figure(figsize=(8,4))
plt.plot(k_values, silhouette_scores, marker='o', label='Silhouette (↑)')
plt.plot(k_values, db_scores, marker='o', label='Davies–Bouldin (↓)')
plt.xlabel("Clusters (k)")
plt.ylabel("Score")
plt.title("K-Means Quick Test Metrics")
plt.legend()
plt.grid(alpha=0.4)
plt.show()

# --- 6. Final quick run ---
kmeans_final = KMeans(n_clusters=best_k, random_state=42, n_init=5)
cluster_labels = kmeans_final.fit_predict(X_scaled)

print("\nQuick test run complete!")
print(pd.Series(cluster_labels).value_counts())
''' #works on small pieces of data, above

'''

#optimized code
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import MiniBatchKMeans, AgglomerativeClustering
from sklearn.metrics import silhouette_score, davies_bouldin_score
import matplotlib.pyplot as plt

# 1. Select numeric features

num_cols = [
    "avg_rating", "recency_days", "helpfulness",
    "total_feedback_count", "total_pos_feedback_count",
    "total_neg_feedback_count", "price_usd"
]

features = [c for c in num_cols if c in customer_df.columns]
X = customer_df[features].dropna()

print(f" Using {len(features)} numeric features: {features}")
print("Data shape before scaling:", X.shape)

# 2. Scale numeric data


scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 3. Run MiniBatch K-Means for k = 3–15 (faster than standard K-Means)

silhouette_scores, db_scores = [], []
k_values = range(3, 16)

print("\n Running MiniBatch K-Means (optimized for large data)...")
for k in k_values:
    kmeans = MiniBatchKMeans(
        n_clusters=k,
        random_state=42,
        batch_size=2048,  # use batches for speed
        n_init=5,         # fewer inits for speed, can raise to 10 if needed
        max_iter=100      # limit iterations for performance
    )
    labels = kmeans.fit_predict(X_scaled)

    sil = silhouette_score(X_scaled, labels)
    db = davies_bouldin_score(X_scaled, labels)

    silhouette_scores.append(sil)
    db_scores.append(db)
    print(f"k={k:2d} | Silhouette={sil:.3f} | Davies–Bouldin={db:.3f}")

# 4. Select best k using combined metric


sil_norm = (silhouette_scores - np.min(silhouette_scores)) / np.ptp(silhouette_scores)
db_norm = (db_scores - np.min(db_scores)) / np.ptp(db_scores)
combined_score = sil_norm - db_norm
best_k = k_values[np.argmax(combined_score)]

print(f"\n Best K (combined silhouette + Davies–Bouldin): {best_k}")




plt.figure(figsize=(10, 5))
plt.plot(k_values, silhouette_scores, marker='o', label='Silhouette (↑ better)')
plt.plot(k_values, db_scores, marker='o', label='Davies–Bouldin (↓ better)')
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Score")
plt.title("MiniBatch K-Means Clustering Metrics")
plt.legend()
plt.grid(True, linestyle='--', alpha=0.5)
plt.show()


# 6. Final K-Means fit using best K


print(f"\n  Running final K-Means with k={best_k}...")
kmeans_final = MiniBatchKMeans(
    n_clusters=best_k,
    random_state=42,
    batch_size=2048,
    n_init=10,
    max_iter=200
)
customer_df["cluster_id"] = kmeans_final.fit_predict(X_scaled)

print("\nCluster distribution (K-Means):")
print(customer_df["cluster_id"].value_counts())


# 7. Hierarchical Clustering (Ward linkage, sampled)


# Hierarchical clustering is O(n²), so we use a 5k sample for performance
sample_size = min(5000, len(X_scaled))
X_sample = X_scaled[np.random.choice(len(X_scaled), sample_size, replace=False)]

print(f"\n Running Agglomerative Clustering (Ward) on {sample_size} samples...")
agg = AgglomerativeClustering(n_clusters=best_k, linkage='ward')
agg_labels = agg.fit_predict(X_sample)

sil_agg = silhouette_score(X_sample, agg_labels)
db_agg = davies_bouldin_score(X_sample, agg_labels)

print(f"\nAgglomerative Clustering (Ward linkage) Results:")
print(f"Silhouette={sil_agg:.3f}, Davies–Bouldin={db_agg:.3f}")

# 8. Summary


print("\n Clustering complete!")
print(f"- Optimal K (K-Means): {best_k}")
print("- K-Means cluster IDs stored in `cluster_id`")
print("- Ward clustering tested on a 5k sample (for comparison).")
''' #doing this on 20k instead of 130k+

'''
Drawbacks
Dataset size	Sampling OK?	Recommended sample size
< 20k rows	 Not needed	Use full data
20k–100k rows	 Yes	10k–20k
100k–1M+ rows	 Strongly recommended	20k–50k
> 1M rows	 Required	30k–100k
'''

'''
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import MiniBatchKMeans, AgglomerativeClustering
from sklearn.metrics import silhouette_score, davies_bouldin_score
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")

# Feature selection
num_cols = [
    "avg_rating", "recency_days", "helpfulness",
    "total_feedback_count", "total_pos_feedback_count",
    "total_neg_feedback_count", "price_usd"
]

features = [c for c in num_cols if c in customer_df.columns]
X = customer_df[features].dropna()

print(f" Using {len(features)} numeric features: {features}")
print(f" Data shape before scaling: {X.shape}")

#  Standardize
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Sampling for faster metric computation
max_sample = 20000
if len(X_scaled) > max_sample:
    sample_idx = np.random.choice(len(X_scaled), max_sample, replace=False)
    X_sample = X_scaled[sample_idx]
    print(f"⚡ Using random sample of {len(X_sample):,} rows for scoring.")
else:
    X_sample = X_scaled
    print(" Using full dataset for scoring (small enough).")

# Run K-Means (MiniBatch for scalability)
silhouette_scores, db_scores = [], []
k_values = range(3, 16)

for k in k_values:
    print(f"Running k={k} ...", end=" ")
    kmeans = MiniBatchKMeans(n_clusters=k, random_state=42, batch_size=1024, n_init=10)
    labels = kmeans.fit_predict(X_scaled)
    labels_sample = labels[sample_idx] if len(X_scaled) > max_sample else labels

    sil = silhouette_score(X_sample, labels_sample)
    db = davies_bouldin_score(X_sample, labels_sample)

    silhouette_scores.append(sil)
    db_scores.append(db)
    print(f" Silhouette={sil:.3f}, Davies–Bouldin={db:.3f}")

# Pick best k (combined normalized metric)
sil_norm = (silhouette_scores - np.min(silhouette_scores)) / np.ptp(silhouette_scores)
db_norm = (db_scores - np.min(db_scores)) / np.ptp(db_scores)
combined_score = sil_norm - db_norm
best_k = k_values[np.argmax(combined_score)]

print(f"\n Optimal K (combined metric): {best_k}")

# Visualize metrics
plt.figure(figsize=(10, 5))
plt.plot(k_values, silhouette_scores, marker='o', label='Silhouette (↑ better)')
plt.plot(k_values, db_scores, marker='o', label='Davies–Bouldin (↓ better)')
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Score")
plt.title("K-Means Clustering Metrics")
plt.legend()
plt.grid(True, linestyle='--', alpha=0.6)
plt.show()

# Final model & cluster assignment
kmeans_final = MiniBatchKMeans(n_clusters=best_k, random_state=42, batch_size=1024, n_init=10)
customer_df["cluster_id"] = kmeans_final.fit_predict(X_scaled)
print("\n Cluster distribution (K-Means):")
print(customer_df["cluster_id"].value_counts())

# Hierarchical clustering (Ward linkage)
print("\nRunning Agglomerative (Ward linkage)...")
agg = AgglomerativeClustering(n_clusters=best_k, linkage='ward')
agg_labels = agg.fit_predict(X_sample)
sil_agg = silhouette_score(X_sample, agg_labels)
db_agg = davies_bouldin_score(X_sample, agg_labels)
customer_df["ward_cluster_id"] = np.nan
customer_df.loc[sample_idx, "ward_cluster_id"] = agg_labels

print(f" Agglomerative (sample) — Silhouette={sil_agg:.3f}, Davies–Bouldin={db_agg:.3f}")

print("\n Done! Cluster IDs saved in:")
print(" - `cluster_id` (K-Means full data)")
print(" - `ward_cluster_id` (Hierarchical sample only)")
'''


#Drawbacks of this
#Dataset size	Sampling OK?	Recommended sample size
#< 20k rows	 Not needed	Use full data
#20k–100k rows	 Yes	10k–20k
#100k–1M+ rows	 Strongly recommended	20k–50k
#> 1M rows	 Required	30k–100k


import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import MiniBatchKMeans, AgglomerativeClustering
from sklearn.metrics import silhouette_score, davies_bouldin_score
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")

# Select numeric features
num_cols = [
    "avg_rating", "recency_days", "helpfulness",
    "total_feedback_count", "total_pos_feedback_count",
    "total_neg_feedback_count", "price_usd"
]

features = [c for c in num_cols if c in customer_df.columns]
X = customer_df[features].dropna()

print(f" Using {len(features)} numeric features: {features}")
print(f" Shape before scaling: {X.shape}")

# Standardize numeric data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Adaptive sampling for metric computation
n = len(X_scaled)
max_sample = min(20000, n)            # cap at 20k
sample_frac = 0.1 if n > 200000 else 0.2
sample_size = int(min(max_sample, n * sample_frac))
sample_idx = np.random.choice(n, sample_size, replace=False)
X_sample = X_scaled[sample_idx]

print(f"⚡ Using {sample_size:,} sampled rows (out of {n:,}) for metric scoring.\n")

# Run MiniBatch K-Means and compute metrics
silhouette_scores, db_scores = [], []
k_values = range(3, 16)

for k in k_values:
    print(f"Running k={k:2d} ...", end=" ")
    kmeans = MiniBatchKMeans(n_clusters=k, random_state=42, batch_size=1024, n_init=10)
    labels_full = kmeans.fit_predict(X_scaled)
    labels_sample = labels_full[sample_idx]

    sil = silhouette_score(X_sample, labels_sample)
    db = davies_bouldin_score(X_sample, labels_sample)

    silhouette_scores.append(sil)
    db_scores.append(db)
    print(f" Silhouette={sil:.3f}, Davies–Bouldin={db:.3f}")

# Determine best K using normalized combination metric
sil_norm = (silhouette_scores - np.min(silhouette_scores)) / np.ptp(silhouette_scores)
db_norm = (db_scores - np.min(db_scores)) / np.ptp(db_scores)
combined_score = sil_norm - db_norm
best_k = k_values[np.argmax(combined_score)]

print(f"\n Optimal K (combined metric): {best_k}")

# Visualize metrics
plt.figure(figsize=(10, 5))
plt.plot(k_values, silhouette_scores, marker='o', label='Silhouette (↑ better)')
plt.plot(k_values, db_scores, marker='o', label='Davies–Bouldin (↓ better)')
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Score")
plt.title("K-Means Clustering Evaluation")
plt.legend()
plt.grid(True, linestyle='--', alpha=0.6)
plt.show()

# Final K-Means clustering on full dataset
print("\nFitting final MiniBatchKMeans on full dataset...")
kmeans_final = MiniBatchKMeans(n_clusters=best_k, random_state=42, batch_size=1024, n_init=10)
labels_final = kmeans_final.fit_predict(X_scaled)

# Safely assign cluster IDs
customer_df["cluster_id"] = np.nan
customer_df.loc[X.index, "cluster_id"] = labels_final

print("\n Cluster distribution (K-Means):")
print(customer_df["cluster_id"].value_counts(dropna=True))

# Hierarchical clustering (Ward linkage) on sample
print("\nRunning Agglomerative (Ward linkage) on sample...")
agg = AgglomerativeClustering(n_clusters=best_k, linkage='ward')
agg_labels = agg.fit_predict(X_sample)
sil_agg = silhouette_score(X_sample, agg_labels)
db_agg = davies_bouldin_score(X_sample, agg_labels)

# Assign Ward labels (sample only)
customer_df["ward_cluster_id"] = np.nan
customer_df.loc[X.index[sample_idx], "ward_cluster_id"] = agg_labels

print(f" Agglomerative (sample) — Silhouette={sil_agg:.3f}, Davies–Bouldin={db_agg:.3f}")

#PCA visulization
print("\n Running PCA for 2D visualization...")
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X_scaled)

pca_sample_idx = np.random.choice(len(X_pca), min(20000, len(X_pca)), replace=False)
plt.figure(figsize=(10, 7))
plt.scatter(
    X_pca[pca_sample_idx, 0],
    X_pca[pca_sample_idx, 1],
    c=np.array(labels_final)[pca_sample_idx],
    cmap="tab10",
    s=10,
    alpha=0.6
)
plt.title(f"PCA 2D Visualization — K-Means Clusters (k={best_k})")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.grid(True, linestyle="--", alpha=0.5)
plt.show()

# Summary
print("\n Clustering complete!")
print(f"- Optimal K (MiniBatchKMeans): {best_k}")
print("- K-Means clusters → `cluster_id` (all data)")
print("- Ward clusters → `ward_cluster_id` (sample only)")
print("- Metrics visualized for both Silhouette & Davies–Bouldin\n")

"""### Task 3: Sentiment Classification with Transformers

"""

customer_df.columns

# TASK 3: COMMUNITY PROFILING & INSIGHTS

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from sklearn.feature_extraction.text import TfidfVectorizer
from textblob import TextBlob
import warnings
warnings.filterwarnings("ignore")

if "sentiment_score" not in customer_df.columns or "sentiment_label" not in customer_df.columns:
    print("  Sentiment columns missing — computing sentiment scores with TextBlob...")

    def get_sentiment(text):
        if not isinstance(text, str) or not text.strip():
            return 0.0
        return TextBlob(text).sentiment.polarity

    def label_sentiment(score, pos_threshold=0.1, neg_threshold=-0.1):
        if score >= pos_threshold:
            return "Positive"
        elif score <= neg_threshold:
            return "Negative"
        else:
            return "Neutral"

    customer_df["sentiment_score"] = customer_df["review_text"].fillna("").astype(str).apply(get_sentiment)
    customer_df["sentiment_label"] = customer_df["sentiment_score"].apply(label_sentiment)

    print("Sentiment columns successfully added.\n")

else:
    print("Sentiment columns already exist.\n")

assert "cluster_id" in customer_df.columns, "Run Task 2 clustering first!"
print(f"Detected {customer_df['cluster_id'].nunique()} clusters.\n")

cluster_summary = (
    customer_df.groupby("cluster_id")
    .agg(
        n_customers=("cluster_id", "size"),
        avg_rating=("avg_rating", "mean"),
        avg_sentiment=("sentiment_score", "mean"),
        pos_rate=("sentiment_label", lambda x: np.mean(x == "Positive")),
        neg_rate=("sentiment_label", lambda x: np.mean(x == "Negative")),
        neu_rate=("sentiment_label", lambda x: np.mean(x == "Neutral")),
        mean_price=("price_usd", "mean"),
        price_std=("price_usd", "std"),
        top_brand=("brand_name", lambda x: x.value_counts().index[0] if len(x.dropna()) else "Unknown"),
        top_product=("product_name", lambda x: x.value_counts().index[0] if len(x.dropna()) else "Unknown"),
        top_skin_type=("skin_type", lambda x: x.value_counts().index[0] if len(x.dropna()) else "Unknown"),
        top_ingredient=("ingredients", lambda x: x.dropna().astype(str).str.split(",").explode().str.strip().value_counts().index[0]
                        if "ingredients" in customer_df.columns and x.notna().any() else "Unknown"),
    )
    .reset_index()
)
display(cluster_summary.head())


sentiment_counts = (
    customer_df.groupby(["cluster_id", "sentiment_label"])
    .size()
    .reset_index(name="count")
)

plt.figure(figsize=(10, 5))
sns.barplot(data=sentiment_counts, x="cluster_id", y="count", hue="sentiment_label")
plt.title("Sentiment Distribution by Cluster")
plt.xlabel("Cluster ID")
plt.ylabel("Count")
plt.legend(title="Sentiment")
plt.grid(axis="y", linestyle="--", alpha=0.6)
plt.show()
'''
def get_top_keywords(texts, n=15):
    vec = TfidfVectorizer(stop_words="english", max_features=3000, min_df=5, ngram_range=(1, 2))
    X = vec.fit_transform(texts)
    scores = np.asarray(X.sum(axis=0)).ravel()
    top_ids = scores.argsort()[-n:][::-1]
    return [vec.get_feature_names_out()[i] for i in top_ids]
'''
def get_top_keywords(texts, n=15):
    vec = TfidfVectorizer(
        stop_words="english",
        max_features=3000,
        min_df=1,
        max_df=0.95,
        ngram_range=(1, 2)
    )
    X = vec.fit_transform(texts)

    scores = np.asarray(X.sum(axis=0)).ravel()
    top_ids = scores.argsort()[-n:][::-1]
    return [vec.get_feature_names_out()[i] for i in top_ids]

cluster_keywords = {}
for cluster_id in sorted(customer_df["cluster_id"].dropna().unique()):
    subset = customer_df.loc[customer_df["cluster_id"] == cluster_id, "review_text"].dropna().astype(str)
    if len(subset) == 0:
        continue
    cluster_keywords[cluster_id] = get_top_keywords(subset, n=15)
    print(f"\n Cluster {cluster_id} top keywords:\n{cluster_keywords[cluster_id]}")

for cluster_id, keywords in cluster_keywords.items():
    text = " ".join(keywords)
    wc = WordCloud(width=800, height=400, background_color="white", colormap="viridis").generate(text)
    plt.figure(figsize=(8, 4))
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    plt.title(f"Cluster {cluster_id}: Keyword Cloud", fontsize=14)
    plt.show()

plt.figure(figsize=(10, 5))
sns.boxplot(data=customer_df, x="cluster_id", y="price_usd")
plt.title("Price Sensitivity by Cluster")
plt.xlabel("Cluster ID")
plt.ylabel("Price (USD)")
plt.grid(axis="y", linestyle="--", alpha=0.5)
plt.show()

personas = []
for _, row in cluster_summary.iterrows():
    sentiment = (
        "Positive" if row["avg_sentiment"] > 0.1 else
        "Negative" if row["avg_sentiment"] < -0.1 else "Neutral"
    )

    if row["mean_price"] < 25 and sentiment == "Positive":
        persona = ("Budget Acne-Focused Students",
                   "Value-conscious, acne-oriented consumers favoring affordable skincare. Respond to discounts and social media influencers.",
                   "TikTok, Instagram Reels")
    elif row["mean_price"] > 50 and sentiment == "Positive":
        persona = ("Premium Anti-Aging Buyers",
                   "Affluent buyers seeking anti-aging and luxury skincare. Respond to clinical credibility and long-form content.",
                   "YouTube, Email Campaigns")
    elif sentiment == "Negative":
        persona = ("Dissatisfied Reviewers",
                   "Critical customers reporting irritation or unmet expectations. Ideal for customer service retention efforts.",
                   "Email Follow-up, Customer Care")
    else:
        persona = ("Balanced Skincare Enthusiasts",
                   "Curious, moderate users exploring diverse brands. Engage through seasonal bundles and recommendations.",
                   "Instagram Ads, Push Notifications")

    personas.append({
        "cluster_id": int(row["cluster_id"]),
        "persona_name": persona[0],
        "dominant_skin_type": row["top_skin_type"],
        "avg_sentiment": round(row["avg_sentiment"], 3),
        "mean_price": round(row["mean_price"], 2),
        "top_brand": row["top_brand"],
        "top_product": row["top_product"],
        "top_ingredient": row["top_ingredient"],
        "marketing_channel": persona[2],
        "description": persona[1]
    })

persona_df = pd.DataFrame(personas)
display(persona_df)

plt.figure(figsize=(10, 5))
sns.scatterplot(
    data=persona_df,
    x="mean_price",
    y="avg_sentiment",
    hue="marketing_channel",
    s=150,
    palette="tab10"
)
plt.title("Community Map: Price vs Sentiment by Marketing Channel")
plt.xlabel("Average Price (USD)")
plt.ylabel("Average Sentiment Score")
plt.grid(alpha=0.4)
plt.show()

cluster_summary.to_csv("cluster_summary.csv", index=False)
persona_df.to_csv("persona_profiles.csv", index=False)
print("\nTask 3 complete — all visualizations generated and results saved.")

"""# Milestone 3"""

# --- FINAL SEGMENT SUMMARY ---

segment_profile = (
    customer_df.groupby("cluster_id")
    .agg(
        cluster_id=("cluster_id", "first"),
        n_customers=("cluster_id", "size"),
        avg_rating=("avg_rating", "mean"),
        avg_sentiment=("sentiment_score", "mean"),
        pos_rate=("sentiment_label", lambda x: np.mean(x == "Positive")),
        neg_rate=("sentiment_label", lambda x: np.mean(x == "Negative")),
        mean_price=("price_usd", "mean"),
        median_price=("price_usd", "median"),
        top_brands=("brand_name", lambda x: x.value_counts().head(5).index.tolist()),
        top_products=("product_name", lambda x: x.value_counts().head(5).index.tolist()),
        top_skin_types=("skin_type", lambda x: x.value_counts().head(3).index.tolist()),
    )
).reset_index(drop=True)

display(segment_profile)

segment_profile.to_csv("segment_profile.csv", index=False)

pain_keywords = {}
for cid in customer_df["cluster_id"].dropna().unique():
    subset = customer_df[
        (customer_df["cluster_id"] == cid) &
        (customer_df["sentiment_label"] == "Negative")
    ]["review_text"]

    if len(subset) == 0:
        pain_keywords[cid] = []
        continue

    texts = subset.dropna().astype(str)

    try:
        pain_keywords[cid] = get_top_keywords(texts, n=15)
    except ValueError:
        pain_keywords[cid] = []

# --- SIGNATURE BRAND + PRODUCT LIFT ANALYSIS ---
def compute_lift(df, cluster_col, item_col, min_support=20):
    """
    Computes item lift relative to global frequency.
    Returns a dataframe of:
    cluster_id | item | cluster_freq | global_freq | lift
    """
    global_freq = df[item_col].value_counts(normalize=True)

    results = []
    for cid in df[cluster_col].dropna().unique():
        sub = df[df[cluster_col] == cid]
        sub_freq = sub[item_col].value_counts(normalize=True)

        joined = pd.concat([sub_freq, global_freq], axis=1, keys=["cluster", "global"]).fillna(0)
        joined["lift"] = joined["cluster"] / joined["global"]

        # apply minimum support in cluster (count not ratio)
        joined = joined[(joined["cluster"] * len(sub) >= min_support)]

        # take top 10 lifts
        joined = joined.sort_values("lift", ascending=False).head(10)

        for idx, row in joined.iterrows():
            results.append([
                cid,
                idx,
                row["cluster"],
                row["global"],
                row["lift"]
            ])

    return pd.DataFrame(
        results,
        columns=["cluster_id", item_col, "cluster_freq", "global_freq", "lift"]
    )



# --- SAFELY REBUILD LIFT TABLES IF MISSING ---
if "signature_brands" not in globals():
    print("Recomputing signature_brands...")
    signature_brands = compute_lift(customer_df, "cluster_id", "brand_name")

if "signature_products" not in globals():
    print("Recomputing signature_products...")
    signature_products = compute_lift(customer_df, "cluster_id", "product_name")

# Guarantee tables exist
assert "signature_brands" in globals(), "signature_brands is missing."
assert "signature_products" in globals(), "signature_products is missing."


# --- PERSONA CARD GENERATION (ERROR-PROOF) ---
persona_cards = []

for _, row in segment_profile.iterrows():
    cid = row["cluster_id"]

    # --- sentiment bucket ---
    if row["avg_sentiment"] > 0.1:
        sentiment_group = "Happy / Satisfied"
    elif row["avg_sentiment"] < -0.1:
        sentiment_group = "Unhappy / Critical"
    else:
        sentiment_group = "Neutral / Mixed"

    # --- price bucket ---
    if row["mean_price"] < 25:
        price_tier = "Budget"
    elif row["mean_price"] < 60:
        price_tier = "Mid-Range"
        # Typo fixed: price-tier should be Mid-range not Mid-Range
    else:
        price_tier = "Premium"

    # --- SAFE SIGNATURE BRAND EXTRACTION ---
    sb = signature_brands[signature_brands.cluster_id == cid]
    sp = signature_products[signature_products.cluster_id == cid]

    signature_brands_list = sb.brand_name.head(5).tolist() if len(sb) > 0 else []
    signature_products_list = sp.product_name.head(5).tolist() if len(sp) > 0 else []

    persona_cards.append({
        "cluster_id": cid,
        "persona_name": f"{price_tier} {sentiment_group} Skincare Users",
        "core_brands": row["top_brands"],
        "favorite_products": row["top_products"],
        "skin_type_cluster": row["top_skin_types"],
        "signature_brands": signature_brands_list,
        "signature_products": signature_products_list,
        "pain_points": pain_keywords.get(cid, []),
        "avg_sentiment": round(row["avg_sentiment"], 3),
        "mean_price": round(row["mean_price"], 2)
    })


persona_cards_df = pd.DataFrame(persona_cards)
display(persona_cards_df)

persona_cards_df.to_csv("persona_cards.csv", index=False)
print("\nPersona cards saved to persona_cards.csv")

from IPython.display import Markdown, display
from ipywidgets import Tab, Output, VBox


# ----------------------------------------
# CLEAN PERSONA TABS WITH NO TOP SPACING
# ----------------------------------------
tabs = Tab()
tab_contents = []

def format_list(items):
    if not items:
        return "None"
    return ", ".join(str(i) for i in items)

for _, row in persona_cards_df.iterrows():
    cid = int(row["cluster_id"])

    out = Output()
    with out:
        # Notice: no leading newline, no trailing newline
        display_markdown = f"""**### Persona {cid}: {row['persona_name']}**

**Core Brands:** {format_list(row['core_brands'])}
**Favorite Products:** {format_list(row['favorite_products'])}
**Skin Types:** {format_list(row['skin_type_cluster'])}
**Signature Brands:** {format_list(row['signature_brands'])}
**Signature Products:** {format_list(row['signature_products'])}
**Pain Points:** {format_list(row['pain_points'])}
"""

        # Render markdown without added spacing
        display(Markdown(display_markdown.strip()))

        # Visuals
        #display(radar_chart(row))
        #display(signature_brands_chart(cid))
        #display(signature_products_chart(cid))

        # Wordcloud
        #fig = pain_point_wordcloud(cid)
        #plt.tight_layout()
        #plt.show()

    # Remove tab margin entirely
    tab_contents.append(VBox([out], layout=dict(margin="0px")))

tabs.children = tab_contents

# Set titles
for i, row in persona_cards_df.iterrows():
    tabs.set_title(i, f"Cluster {int(row['cluster_id'])}")

tabs
